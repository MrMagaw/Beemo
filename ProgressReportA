Progress Report A

The Current Build:

The current most effective structure implemented uses Monte Carlo simulation from each unique available position to determine the best move from the given board. The effectiveness of the simulations is improved by using a policy to play the simulations. The policy uses hand values to staticly evaluate all positions in order to compare them, and the values themselves are trained using monte carlo estimations. 

The Monte Carlo Structure:

The current implementation of Monte Carlo simulation is the simplest, most straightforward technique available. From a given board and card each possible position evaluated by playing n games from each position and assigning each position a value equal to the average score of the games played. As described above, the games are not played randomly but rather according to a best first/greedy policy that will be illuminated below. An equal number of simulations are run from each position in the board in order to ensure that the averages are comparable. 

Symmetries:

When evaluating the positions on the board, certain positions are abstractly equivalent, these positions are not evaluated to avoid redundancy. For example, an intersecting row and column representing a position such as 

Row: 5H -- -- -- -- 
Col: 4S -- -- -- -- 

are analagous to a row and column such as

Row: 9D -- -- -- --
Col: -- -- AC -- --

Because the card ordering in the hands does not matter and each contains only a high card, so the score for only one of these position pairs must be retained when simulating games from positions in monte carlo. 

The Evaluation Policy:

The policy used when simulating games is, as described, based on static position evaluation and greedy play. From a board and card, each of the possible positions that can be played are evaluated based on their respective intersecting row and column hands. To illustrate this clearly; the static evaluation of a position is the difference in value of the static evaluation of the hands represented by its row and column before and after the .

For the board and card:

2C -- 5A -- --
-- -- -- TA --
-- -- -- -- --
-- 3H 4A -- --
-- -- -- -- --  and 2H

The position ( 0 , 1 ) is represented by the two hands:

Row: 2C -- 5A -- --
Col: -- -- -- 3H --

And the evaluation of the position is the sum of the evaluation of the two hands after the card has been placed in each of them minus the sum of the evaluation of the two hands before the card has been placed in them.

So:

RowA: 2C -- 5A -- -- = 0
ColA: -- -- -- 3H -- = 0

RowB: 2C 2H 5A -- -- = 5 //some score for the double
ColB: -- 2H -- 3H -- = 4.8 //some score for the possible flush

Pos (0,1) = (5 + 4.8) - (0 + 0)


When comparing positions on the same, it is appropriate to look at only their local hands because only the values of these hands change when the new card is placed, the values of the rest of the board remain the same and so can be assumed to be net 0. This static comparison does not take into account which cards remain in the deck. IE, a hand with 3 Aces in a game where the other Ace has already been played on the board (meaning the current hand has no possibility of a four of a kind) is scored the same as a hand with three aces where the fourth ace remains unplayed. This is a compromise of information that significantly reduces complexity while retaining enough information for relatively 'good' play. 

Hand Patterns:

In order to analyze hands, compare them for redundancy and score them, each hand is reduced to an abstract representation of the hand that retains all crucial information. When developing this abstract hand representation, the minimum amount of information needed is enough to describe all complete and partial poker hand structure present in the hand (high card, pair, flush... etc). We call these abstractions hand patterns. The first principal is order: the order of the cards in a hand does not matter to the evaluation of the hand. Second, only enough information about the suits of the cards must be retained to determine whether a hand is or is not flush capable and the number of cards in the hand, the type of suite is not important. Similiarly, the actual ranks of the cards are also not important, only if there are pairs, three of a kinds or four of a kinds present. The last peice of information needed is the possibility of a straight. This is how we encoded this information where the first three items are set by a single bit and the last 5 are set by 3 bits:

[isCol][hasFlush][hasStraight][nSingleCards][nPairs][nTwoPairs][n3Kind][n4Kind]

The hands:

Col: 4H -- 4S 5S --
Row: 2C -- 7C -- 5C 

would be encoded:

[1][0][0][1][1][0][0][0]
[0][1][1][3][0][0][0][0]

This information is stored as 32 bit ints so they are uniquely hashable and comparable. The only hand information that is lost is for identifying a Royal Flush, we chose not to include this because the [lilliputian] performance increase is outweighed by the computational time cost. Each pattern is mapped to a score (reward) that is used as the static evaluation of any hand that matches the pattern. 

The Pattern Scores and Training Regimen:

The patterns have no hard coded values, they are all trained using Monte Carlo Simulation to approximate a value. Values are trained by keeping track of patterns that occur in the hands of a game and then averaging the final score that patterns result in over the number of games the patterns occur in. So in each trial of training, a game is played on a board. This board contains ten hands. Each hand will be represented by 4 patterns throughout the game (empty hands are not patterned), one for each new card added to the hand. Each pattern receives the score of the final hand, which is evaluated according the given scoring system (american/biritish/random/other). 

A hand in a game might progress as follows:

5H -- -- -- --
5H 5S -- -- --
5H 5S 3S -- --
5H 5S 3S 3C --
5H 5S 3S 3C 5D

and receive a final [american] score of 25 for a full house. Every intermediate pattern for all of the partial hands now also receives a score of 25. This means the initial hand containing only a high card is valued at 25.

A subsequent hand might go: 

5H -- -- -- --
5H 6S -- -- --
5H 6S 3S -- --
5H 6S 3S 4C --
5H 6S 3S 4C 5D

and reveive a final score of 0 for only having a high card. The high card pattern would now have a score of 25 + 0 for two games, so it's average value is 25/2 = 12.5.

This process is repeated  for tens of thousands of games and generally finds equilibrium after 50 000 games or so. Equilibrium seems to be reached at local maxima so restarts are employed after an interval of games to acheive a new equilibrium.

Additionally, a feedback process which improves the score significantly, but on which we are cloudy on the machinations of, is used to guide the game play; games are played according to the greedy policy described above. Initially because there are no pattern scores to guide the play, games are played randomly, but then as patterns values for patterns are formed, play improves which in turn improves the performance of the values determined. 

Moving Fowrward:

Having now established a generalized static evaluation and greedy play policy which performs incredibly fast, we intend on implementing more in depth Monte Carlo techniques. 
    
1. UCT Policy: Implement a policy that appropriately trades exploration for exploitation, current attempts have only decreased performance

2. Tree Search: Extend the monte carlo structure into MCTS to avoid evaluating nodes multiple times. 

3. On-the-fly-Training: Use the results of monte carlo simulation to reevaluate hand patterns.

4. Seeded Nodes: Seed the initial nodes according to their static evaluations. 






